{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA_Document_Clustering",
      "provenance": [],
      "authorship_tag": "ABX9TyN7UaKD+VJf4o3zd8iqqbJp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwinjosep/swm-patent-dashboard/blob/master/LDA_Document_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9siVqKpqfhIe",
        "colab_type": "text"
      },
      "source": [
        "# CSE 573 - Semantic Web Mining | Spring 2020 | Group 19\n",
        "# Document clustering and visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRabyEwIf3YV",
        "colab_type": "text"
      },
      "source": [
        "**Setting up of environment and imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urNwPOVYe928",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "011a3d2f-96c1-44ed-cc59-8f8b409ef7e2"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from scipy.stats import entropy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC-y1D3xfJll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "1459f783-39a0-46b0-9086-16a5d67c80b6"
      },
      "source": [
        "''' \n",
        "Run this command before processing\n",
        "nltk.download('punkt')\n",
        "'''\n",
        "\n",
        "'''\n",
        "Constants\n",
        "'''\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "NUMBER_OF_SIMILAR_DOCS = 15\n",
        "NUMBER_OF_TOPICS = 10\n",
        "OUTPUT_PATH_DIR = '/content/gdrive/results/'\n",
        "\n",
        "'''\n",
        "Variables\n",
        "'''\n",
        "stop_words = stopwords.words('english')\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_8r-Yw4gBoU",
        "colab_type": "text"
      },
      "source": [
        "**Use Google drive to load and store data files. \n",
        "Mounting drive as storage.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfAC0To3fSPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwYIo3nqgoOF",
        "colab_type": "text"
      },
      "source": [
        "**Funtion definitions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9KxVTCggQVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initial_clean(text):\n",
        "    \"\"\"\n",
        "    Function to clean text of websites, email addresses and any punctuation\n",
        "    We also lower case the text\n",
        "    \"\"\"\n",
        "    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n",
        "    text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n",
        "    text = text.lower()  # lower case the text\n",
        "    text = nltk.word_tokenize(text)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQGsFcmhgS_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stop_words(text):\n",
        "    \"\"\"\n",
        "    Function that removes all stopwords from text\n",
        "    \"\"\"\n",
        "    return [word for word in text if word not in stop_words]\n",
        "\n",
        "\n",
        "def stem_words(text):\n",
        "    \"\"\"\n",
        "    Function to stem words, so plural and singular are treated the same\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = [stemmer.stem(word) for word in text]\n",
        "        text = [word for word in text if len(word) > 1]  # make sure we have no 1 letter words\n",
        "    except IndexError:  # the word \"oed\" broke this, so needed try except\n",
        "        pass\n",
        "    return text\n",
        "\n",
        "\n",
        "def apply_all(text):\n",
        "    \"\"\"\n",
        "    This function applies all the functions above into one\n",
        "    \"\"\"\n",
        "    return stem_words(remove_stop_words(initial_clean(text)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix1E1Po8gT5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_lda(data):\n",
        "    \"\"\"\n",
        "    This function trains the lda model\n",
        "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
        "    We also do 2 passes of the data since this is a small dataset, so we want the distributions to stabilize\n",
        "    \"\"\"\n",
        "    chunksize = 300\n",
        "    dictionary = corpora.Dictionary(data['tokenized'])\n",
        "    corpus = [dictionary.doc2bow(doc) for doc in data['tokenized']]\n",
        "    t1 = time.time()\n",
        "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
        "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
        "    lda = LdaModel(corpus=corpus, num_topics=NUMBER_OF_TOPICS, id2word=dictionary,\n",
        "                   alpha=1e-2, eta=0.5e-2, chunksize=chunksize, minimum_probability=0.0, passes=3, dtype=np.float64)\n",
        "    t2 = time.time()\n",
        "    print(\"Time to train LDA model on \", len(data), \"articles: \", (t2 - t1) / 60, \"min\")\n",
        "    return dictionary, corpus, lda\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXIJ5JvagVcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jensen_shannon(query, matrix):\n",
        "    \"\"\"\n",
        "    This function implements a Jensen-Shannon similarity\n",
        "    between the input query (an LDA topic distribution for a document)\n",
        "    and the entire corpus of topic distributions.\n",
        "    It returns an array of length M where M is the number of documents in the corpus\n",
        "    \"\"\"\n",
        "    # lets keep with the p,q notation above\n",
        "    p = query[None, :].T  # take transpose\n",
        "    q = matrix.T  # transpose matrix\n",
        "    print(p.shape)\n",
        "    print(q.shape)\n",
        "    m = 0.5 * (p + q)\n",
        "    return np.sqrt(0.5 * (entropy(p, m) + entropy(q, m)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I_PLqPdgZbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_most_similar_documents(query, matrix, k=NUMBER_OF_SIMILAR_DOCS):\n",
        "    \"\"\"\n",
        "    This function implements the Jensen-Shannon distance above\n",
        "    and retruns the top k indices of the smallest jensen shannon distances\n",
        "    \"\"\"\n",
        "    sims = jensen_shannon(query, matrix)  # list of jensen shannon distances\n",
        "    return np.partition(sims, -k)[-k:], sims.argsort()[\n",
        "                                        :k]  # the top k positional index of the smallest Jensen Shannon distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvizO_nkg0S4",
        "colab_type": "text"
      },
      "source": [
        "**Read data from full_text.csv and store them into parts for faster processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI5yeveGhEIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/gdrive/My Drive/full_text.csv', usecols=['text', 'patent'])\n",
        "split_df = np.split(df, [10000, 20000, 30000, 40000, 50000, 60000, 70000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWcjlnv8iRuq",
        "colab_type": "text"
      },
      "source": [
        "Call the *clean_and_tokenize()* method for all the dataframe splits and store in separate pickle files so that they can be run parallely in batches for faster processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjFTxJHmhNGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_and_tokenize():\n",
        "    df = df[df['text'].map(type) == str]\n",
        "    df.dropna(axis=0, inplace=True, subset=['text'])\n",
        "    # shuffle the data\n",
        "    df = df.sample(frac=1.0)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    # print(df.shape)\n",
        "    # print(df.head())\n",
        "\n",
        "    print(\"==> Cleaning and tokenizing\")\n",
        "    t1 = time.time()\n",
        "    df['tokenized'] = df['text'].apply(apply_all)\n",
        "    t2 = time.time()\n",
        "    print(\"Time to clean and tokenize\", len(df), \"articles:\", (t2 - t1) / 60, \"min\")\n",
        "    df.to_pickle('/content/gdrive/My Drive/tokenized_df.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6Dr80Gyi-Nn",
        "colab_type": "text"
      },
      "source": [
        "Read data from different parts or file and concatenate them into a single dataframe for further processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm0jhmxci7_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = pd.read_pickle('/content/gdrive/My Drive/tokenized_df1.pickle')\n",
        "df1.drop(['text'], axis=1, inplace=True)\n",
        "df2 = pd.read_pickle('/content/gdrive/My Drive/tokenized_df2.pickle')\n",
        "df2.drop(['text'], axis=1, inplace=True)\n",
        "df3 = pd.read_pickle('/content/gdrive/My Drive/tokenized_df3.pickle')\n",
        "df3.drop(['text'], axis=1, inplace=True)\n",
        "df4 = pd.read_pickle('/content/gdrive/My Drive/tokenized_df4.pickle')\n",
        "df4.drop(['text'], axis=1, inplace=True)\n",
        "df5 = pd.read_pickle('/content/gdrive/My Drive/tokenized_df5.pickle')\n",
        "df5.drop(['text'], axis=1, inplace=True)\n",
        "df6 = pd.read_pickle('/content/gdrive/My Drive/tokenized_df6.pickle')\n",
        "df6.drop(['text'], axis=1, inplace=True)\n",
        "df7 = pd.read_pickle('/content/gdrive/My Drive/tokenized_df7.pickle')\n",
        "df7.drop(['text'], axis=1, inplace=True)\n",
        "tokenized_df = pd.concat([df1, df2, df3, df4, df5, df6, df7])\n",
        "print(tokenized_df.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4KnVVwejTas",
        "colab_type": "text"
      },
      "source": [
        "Delete the smaller dataframes to reduce usage of RAM space in colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKeb-cbRjT0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del [df7, df6, df5, df4, df3, df2, df1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyQd-iNqgkvz",
        "colab_type": "text"
      },
      "source": [
        "Build corpus of all the words from the tokenized text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5X3x8NDjiGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    df = tokenized_df\n",
        "    print(\"==> Building corpus\")\n",
        "    # first get a list of all words\n",
        "    all_words = [word for item in list(df['tokenized']) for word in item]\n",
        "    # use nltk fdist to get a frequency distribution of all words\n",
        "    fdist = FreqDist(all_words)\n",
        "    # print(len(fdist))  # number of unique words\n",
        "\n",
        "    # choose k and visually inspect the bottom 10 words of the top k\n",
        "    k = 15000\n",
        "\n",
        "    # define a function only to keep words in the top k words\n",
        "    top_k_words, _ = zip(*fdist.most_common(k))\n",
        "    top_k_words = set(top_k_words)\n",
        "\n",
        "    def keep_top_k_words(text):\n",
        "        return [word for word in text if word in top_k_words]\n",
        "\n",
        "    df['tokenized'] = df['tokenized'].apply(keep_top_k_words)\n",
        "    del all_words\n",
        "    del top_k_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9Ro-kHTjzgO",
        "colab_type": "text"
      },
      "source": [
        "**Train the LDA model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCY9j-pLjzwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    train_df = df\n",
        "    train_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    print(\"==> Performing LDA\")\n",
        "    t3 = time.time()\n",
        "    dictionary, corpus, lda = train_lda(train_df)\n",
        "    t4 = time.time()\n",
        "    print(\"Time to perfrom LDA:\", (t3 - t4) / 60, \"min\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bk2t6iVj66E",
        "colab_type": "text"
      },
      "source": [
        "Save the LDA model, dictionary and corpus so that they can be reused"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1wOf0-Kj7Az",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "filename = '/content/gdrive/My Drive/SWM_Final/finalized_lda_model.sav'\n",
        "pickle.dump(lda, open(filename, 'wb'))\n",
        "\n",
        "dictionary.save('/content/gdrive/My Drive/SWM_Final/lda_dict.pickle')\n",
        "dictionary.save_as_text('/content/gdrive/My Drive/SWM_Final/lda_dict.txt')\n",
        "\n",
        "with open('/content/gdrive/My Drive/SWM_Final/lda_corpus.data', 'wb') as filehandle:\n",
        "    # store the data as binary data stream\n",
        "    pickle.dump(corpus, filehandle)\n",
        "    \n",
        "train_df.to_pickle('/content/gdrive/My Drive/training_df.pickle')    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yJuKhZmkYDP",
        "colab_type": "text"
      },
      "source": [
        "Load the saved model, corpus and training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA6uTz3kkVpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('/content/gdrive/My Drive/SWM_Final/lda_corpus.data', 'rb') as filehandle:\n",
        "      # read the data as binary data stream\n",
        "      corpus = pickle.load(filehandle)\n",
        "\n",
        "with open('/content/gdrive/My Drive/SWM_Final/finalized_lda_model.sav', 'rb') as filehandle:\n",
        "  lda = pickle.load(filehandle)  \n",
        "\n",
        "train_df = pd.read_pickle('/content/gdrive/My Drive/SWM_Final/train_df.pickle')\n",
        "train_df = train_df.apply(pd.to_numeric)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oECnqtrHkjjY",
        "colab_type": "text"
      },
      "source": [
        "Get the topic distribution matrix of all the training documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHOE4KHQkjGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in lda[corpus]])\n",
        "print(doc_topic_dist.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1iSNdtLksU3",
        "colab_type": "text"
      },
      "source": [
        "Get the top most cited 1000 patents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ethfeik1ksaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    from ast import literal_eval\n",
        "\n",
        "    patent_citations_df = pd.read_csv('/content/gdrive/My Drive/SWM_Final/patent_dict_1000.csv', usecols=['patent', 'number', 'citations'],\n",
        "                                      nrows=1000)\n",
        "    patent_citations_df.loc[:, 'citations'] = patent_citations_df.loc[:, 'citations'].apply(lambda x: literal_eval(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmShXQj4lLwz",
        "colab_type": "text"
      },
      "source": [
        "Compute similarities of all the top 1000 patents with the patents that are citing them and return the top 15 most similar documents and write them into a csv file called 'lda_similarities.csv' so that it can be used for Visualisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GZcnfy5lKnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " with open('/content/gdrive/My Drive/SWM_Final/lda_similarities.csv', 'w') as fo:\n",
        "    for index, row in patent_citations_df.iterrows():\n",
        "        patent = row.values.tolist()[0]\n",
        "        patent_index = train_df.index[train_df['patent'].isin([patent])]\n",
        "        if len(patent_index.values.tolist()) == 0:\n",
        "          continue\n",
        "        patent_topic_dist = doc_topic_dist[patent_index][0]\n",
        "        citations = list(map(int, row.values.tolist()[2]))\n",
        "        indexes = train_df.index[train_df['patent'].isin(citations)].tolist()\n",
        "        cited_patents =  train_df[train_df['patent'].isin(citations)]\n",
        "        cited_patents.reset_index(drop=True, inplace=True)\n",
        "        cited_topic_dist = doc_topic_dist[[indexes], :][0]\n",
        "        similarity_values, most_sim_ids = get_most_similar_documents(patent_topic_dist, cited_topic_dist)\n",
        "        most_similar_df = cited_patents[cited_patents.index.isin(most_sim_ids)]['patent']\n",
        "        patient_df = most_similar_df.to_numpy(dtype=str)\n",
        "        sim_values = np.vstack((patient_df, similarity_values)).T.tolist()\n",
        "        similarity = get_similarity_format(sim_values)\n",
        "        fo.write(str(patent) + \",\" + str(similarity).strip('[]') + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ETD4BVFlxiy",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOgu9vpWl1C3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.corpora.dictionary import Dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONODlCoMl5d2",
        "colab_type": "text"
      },
      "source": [
        "Load the saved LDA model, corpus and dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6OF9wnYl4qR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/gdrive/My Drive/SWM_Final/finalized_lda_model.sav', 'rb') as filehandle:\n",
        "      # read the data as binary data stream\n",
        "      ldamodel = pickle.load(filehandle)\n",
        "\n",
        "dictionary = Dictionary.load('/content/gdrive/My Drive/SWM_Final/lda_dict.pickle')\n",
        "\n",
        "with open('/content/gdrive/My Drive/SWM_Final/lda_corpus.data', 'rb') as filehandle:\n",
        "      # read the data as binary data stream\n",
        "      corpus = pickle.load(filehandle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxjp5J_El9oj",
        "colab_type": "text"
      },
      "source": [
        "Calculate the u_mass score for getting the topic coherence of LDA model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPfmnLJol91q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "goodcm = CoherenceModel(model=ldamodel, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
        "print(goodcm.get_coherence())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}